{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# in a notebook, load the tensorboard extension, not needed for scripts\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having taken a look at these, we will look at how this can be used to track the metrics (loss, accuracy etc.) of a subclassed model's training. For this we will implement the train and test step as internal methods of the model and have the loss, optimizer and metrics as attributes of the model. This moves us one step closer to understanding how the compile and fit methods of tf.keras.Model work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: Using TensorBoard to store loss and accuracy of a subclassed model\n",
    "\n",
    "We will define a subclassed feed forward fully connected model and store loss and accuracy for both training and validation data to the TensorBoard. \n",
    "\n",
    "To do this in a clean way, we implement the keras metrics that keep track of loss and accuracy in each epoch for us as part of the model. We also define the train and test steps as methods inside the model rather than as external functions. Doing so will move us one step closer to being able to use the in-built training and evaluation methods that come with Tensorflow/Keras, that is the compile and fit methods, **which we do not yet allow for the homeworks**.\n",
    "\n",
    "To use train_step and test_step as methods of the model, we need to have the loss-function, the metrics, and the optimizer as parts of the model, which is why we define them in the init method.\n",
    "\n",
    "Note that we need to update the metrics after each training example and reset the metrics after each epoch or before evaluating our model on the validation data set.\n",
    "\n",
    "Also note that the metrics_list contains a mean metric for the loss, which does not take targets and predictions as arguments in its update_state method, but just a scalar. For this reason, we treat it differently from the remaining metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.metrics_list = [\n",
    "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "                        tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
    "                       ]\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)   \n",
    "        \n",
    "        # define layers\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.layer1 = tf.keras.layers.Dense(32,activation=\"relu\")\n",
    "        self.layer2 = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.layer3 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.layer4 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.output_layer = tf.keras.layers.Dense(10, activation=None)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # flatten images to vectors\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        out = self.output_layer(x)\n",
    "       \n",
    "        return out\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        x, targets = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(x, training=True)\n",
    "            \n",
    "            loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets,predictions)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "\n",
    "        x, targets = data\n",
    "        predictions = self(x, training=False)\n",
    "        loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
    "\n",
    "        self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets, predictions)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tfds.load(\"fashion_mnist\", as_supervised=True)\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"test\"]\n",
    "\n",
    "train_ds = train_ds.map(lambda x,y: (x/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                        num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.map(lambda x,y: (x/255, tf.one_hot(y, 10, dtype=tf.float32)),\\\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(5000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ffn_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            multiple                  25120     \n",
      "                                                                 \n",
      " dense_26 (Dense)            multiple                  2112      \n",
      "                                                                 \n",
      " dense_27 (Dense)            multiple                  8320      \n",
      "                                                                 \n",
      " dense_28 (Dense)            multiple                  33024     \n",
      "                                                                 \n",
      " dense_29 (Dense)            multiple                  2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,152\n",
      "Trainable params: 71,146\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "model = FFN()\n",
    "\n",
    "# run model on input once so the layers are built\n",
    "model(tf.keras.Input((28,28,1)));\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the file-writers for the training\n",
    "\n",
    "We store the tensorboard logs to a folder with a meaningful name (e.g. name of training run + date and time). Additionally, when running experiments, you want to save a config file that can be associated with these logs, containing all information about the architecture and hyperparameters that were used. To be extra sure, you could also make a copy of the code that was used. Not knowing which settings lead to which results should be avoided by all means.\n",
    "\n",
    "- We create a train writer and a validation writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the training loop\n",
    "\n",
    "Note that you need to re-run the above cell (and hence update the time-stamp) if you don't want to over-write the data of the previous training-run.\n",
    "\n",
    "If you use keras metrics, do not forget to reset the states between train and validation and between epochs.\n",
    "We use metric.update_states(...) to update a metric. This usually means we update the running average with the new value. There also exist keras metrics that can also compute scores such as CategoricalAccuracy, TopKCategoricalAccuracy.\n",
    "\n",
    "We use TQDM to see the progress of each epoch and the estimate of how much time it will take.\n",
    "\n",
    "Instead of looking at the printed losses and accuracies, we can look at the TensorBoard plots which will be updated after every epoch. This requires us to open and load the tensorboard *before* starting the training or to open the tensorboard from a terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import tqdm\n",
    "\n",
    "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        # Training:\n",
    "        \n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "            \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics (requires a reset_metrics method in the model)\n",
    "        model.reset_metrics()    \n",
    "        \n",
    "        # Validation:\n",
    "        for data in val_ds:\n",
    "            metrics = model.test_step(data)\n",
    "        \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "                    \n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        model.reset_metrics()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25846), started 0:47:12 ago. (Use '!kill 25846' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8414769af79ce8df\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8414769af79ce8df\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:06<00:00, 274.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.5349565148353577', 'acc: 0.8064833283424377', 'top-3-acc: 0.9739333391189575']\n",
      "['val_loss: 0.47390884160995483', 'val_acc: 0.8230999708175659', 'val_top-3-acc: 0.9830999970436096']\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 346.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.3938239812850952', 'acc: 0.8550666570663452', 'top-3-acc: 0.9866833090782166']\n",
      "['val_loss: 0.40563368797302246', 'val_acc: 0.8504999876022339', 'val_top-3-acc: 0.9858999848365784']\n",
      "\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 324.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.3571796417236328', 'acc: 0.8676666617393494', 'top-3-acc: 0.9885500073432922']\n",
      "['val_loss: 0.38291600346565247', 'val_acc: 0.8569999933242798', 'val_top-3-acc: 0.9872000217437744']\n",
      "\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 329.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.3339930474758148', 'acc: 0.876466691493988', 'top-3-acc: 0.9891833066940308']\n",
      "['val_loss: 0.3967644274234772', 'val_acc: 0.8518999814987183', 'val_top-3-acc: 0.9864000082015991']\n",
      "\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 332.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.32334399223327637', 'acc: 0.8801500201225281', 'top-3-acc: 0.9901333451271057']\n",
      "['val_loss: 0.3744821846485138', 'val_acc: 0.8608999848365784', 'val_top-3-acc: 0.9872999787330627']\n",
      "\n",
      "\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 335.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.30862173438072205', 'acc: 0.8839666843414307', 'top-3-acc: 0.9910333156585693']\n",
      "['val_loss: 0.3743337094783783', 'val_acc: 0.8618000149726868', 'val_top-3-acc: 0.9879000186920166']\n",
      "\n",
      "\n",
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 333.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.2978439927101135', 'acc: 0.888949990272522', 'top-3-acc: 0.9920166730880737']\n",
      "['val_loss: 0.3670954704284668', 'val_acc: 0.8669999837875366', 'val_top-3-acc: 0.9879999756813049']\n",
      "\n",
      "\n",
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 294/1875 [00:00<00:04, 320.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# run the training loop \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_loop(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m      3\u001b[0m                 train_ds\u001b[39m=\u001b[39;49mtrain_ds, \n\u001b[1;32m      4\u001b[0m                 val_ds\u001b[39m=\u001b[39;49mval_ds, \n\u001b[1;32m      5\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m                 train_summary_writer\u001b[39m=\u001b[39;49mtrain_summary_writer, \n\u001b[1;32m      7\u001b[0m                 val_summary_writer\u001b[39m=\u001b[39;49mval_summary_writer)\n",
      "Cell \u001b[0;32mIn [49], line 11\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Training:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# logging the validation metrics to the log file which is used by tensorboard\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run the training loop \n",
    "training_loop(model=model, \n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=10, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading a subclassed model\n",
    "\n",
    "Because training deep neural networks can take multiple days, weeks or even months, we want to save checkpoints in between. This is especially useful if you use Google Colab and you save the model directly to your Google Drive folder. That way you don't lose any progress if your runtime gets closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 322.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.28654325008392334', 'acc: 0.8924826979637146', 'top-3-acc: 0.9921514987945557']\n",
      "['val_loss: 0.4096531569957733', 'val_acc: 0.8633000254631042', 'val_top-3-acc: 0.9876999855041504']\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 329.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.27822718024253845', 'acc: 0.893750011920929', 'top-3-acc: 0.9928666949272156']\n",
      "['val_loss: 0.355771541595459', 'val_acc: 0.8743000030517578', 'val_top-3-acc: 0.9884999990463257']\n",
      "\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:05<00:00, 321.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.26944810152053833', 'acc: 0.8980500102043152', 'top-3-acc: 0.9928833246231079']\n",
      "['val_loss: 0.34681302309036255', 'val_acc: 0.8765000104904175', 'val_top-3-acc: 0.9882000088691711']\n",
      "\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 671/1875 [00:02<00:03, 329.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m loaded_model\u001b[39m.\u001b[39mload_weights(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved_model_\u001b[39m\u001b[39m{\u001b[39;00mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m);\n\u001b[1;32m     15\u001b[0m \u001b[39m# continue training (but: optimizer state is lost)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# run the training loop \u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m training_loop(model\u001b[39m=\u001b[39;49mloaded_model, \n\u001b[1;32m     19\u001b[0m                 train_ds\u001b[39m=\u001b[39;49mtrain_ds, \n\u001b[1;32m     20\u001b[0m                 val_ds\u001b[39m=\u001b[39;49mval_ds, \n\u001b[1;32m     21\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m     22\u001b[0m                 train_summary_writer\u001b[39m=\u001b[39;49mtrain_summary_writer, \n\u001b[1;32m     23\u001b[0m                 val_summary_writer\u001b[39m=\u001b[39;49mval_summary_writer)\n",
      "Cell \u001b[0;32mIn [49], line 11\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Training:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# logging the validation metrics to the log file which is used by tensorboard\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_handler\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_traceback_filtering_enabled():\n\u001b[1;32m    141\u001b[0m       \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    142\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mNameError\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# In some very rare cases,\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39m# accessible from inside this function\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:47\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_traceback_filtering_enabled\u001b[39m():\n\u001b[1;32m     34\u001b[0m   \u001b[39m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[39m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m    was called).\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m   value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(_ENABLE_TRACEBACK_FILTERING, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m   \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save the model with a meaningful name\n",
    "model.save_weights(f\"saved_model_{config_name}\", save_format=\"tf\")\n",
    "\n",
    "# load the model:\n",
    "# instantiate a new model from our CNN class\n",
    "loaded_model = FFN()\n",
    "\n",
    "# build the model\n",
    "inp= tf.keras.Input((28,28,1))\n",
    "loaded_model(inp)\n",
    "\n",
    "# load the model weights to continue training. \n",
    "loaded_model.load_weights(f\"saved_model_{config_name}\");\n",
    "\n",
    "# continue training (but: optimizer state is lost)\n",
    "\n",
    "# run the training loop \n",
    "training_loop(model=loaded_model, \n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=10, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54ff86533a6a943eb33cb0954e5964c6e356fb8134919fff31cf4713965c9c7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
